{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import foolbox\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "datapath = '/home/user/datasets/ImageNet/'\n",
    "traindir = os.path.join(datapath, 'train')\n",
    "labeldir = '/home/user/datasets/ImageNet/class_to_idx.txt'\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    traindir,\n",
    "    transforms.Compose([\n",
    "#         transforms.Resize(224),\n",
    "       transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "   #      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                 \t     std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "#     num_workers=1, pin_memory=True, sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet101 = models.resnet101(pretrained=True).eval()\n",
    "if torch.cuda.is_available():\n",
    "    resnet101 = resnet101.cuda()\n",
    "else:\n",
    "    print('===============')\n",
    "mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "fmodel = foolbox.models.PyTorchModel(\n",
    "    resnet101, bounds=(0, 1), num_classes=1000, preprocessing=(mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from abdm.abdm import ABDM\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_id=[]        #ori images ID list\n",
    "img_ori=[]       #ori images list\n",
    "img_adv=[]       #adv images list\n",
    "img_label=[]     #ori images labels list\n",
    "stn_image=[]     #stn images list  \n",
    "\n",
    "wrong_oriimg=0\n",
    "right_advimg=0\n",
    "wrong_advimg=0\n",
    "right_stnimg=0\n",
    "wrong_stnimg=0\n",
    "list=[1007112, 1007113,1007114, 1007115, 1007116,1007117,1007118,1007119,1007120,1007121,1007122,1007123, 1007126,1007127, 1007128, 1007129, 1007132, 1007138, 1007143, 1007145, 1007147, 1007153, 1007167, 1007171,1007172,1007173,1007174,1007175,1007176, 1007177,1007178, 1007179, 1007185, 1007187, 1007194, 1007196, 1007197, 1007198, 1007202, 1007204, 1007207, 1007212, 1007213, 1007215, 1007221, 1007223, 1007226, 1007229, 1007232, 1007234, 1007236, 1007242, 1007243, 1007245, 1007247, 1007250, 1007258, 1007261, 1007262, 1007266, 1007271, 1007273, 1007276, 1007278, 1007280, 1007283, 1007285, 1007289, 1007294, 1007300, 1007307, 1007310, 1007312, 1007313, 1007315, 1007317, 1007321, 1007323, 1007327, 1007330, 1007332, 1007336, 1007337, 1007341, 1007342, 1007347, 1007359, 1007360, 1007363, 1007366, 1007369, 1007370, 1007374, 1007378, 1007380, 1007382, 1007386, 1007389, 1007392, 1007400 ]\n",
    "\n",
    "for num in list:\n",
    "    image, target = train_dataset[num]\n",
    "    image= np.array(image)\n",
    "    print('predicted class', np.argmax(fmodel.predictions(image)),', ground truth class',target)\n",
    "    tempclass1=str(np.argmax(fmodel.predictions(image)))\n",
    "    tempclass2=str(target)\n",
    "    if(tempclass1!=tempclass2):\n",
    "        wrong_oriimg=wrong_oriimg+1\n",
    "        continue\n",
    "        \n",
    "    #dp_attack = foolbox.attacks.FGSM(fmodel)\n",
    "    dp_attack = foolbox.attacks.deepfool.DeepFoolAttack(fmodel, distance=foolbox.distances.Linfinity)\n",
    "    #dp_attack = foolbox.attacks.PGD(fmodel, distance=foolbox.distances.Linfinity)\n",
    "    adversarial = dp_attack(image, target)\n",
    "    try:\n",
    "        print('adversarial class', np.argmax(fmodel.predictions(adversarial)))\n",
    "    except: \n",
    "        wrong_advimg=wrong_advimg+1\n",
    "        print('error')\n",
    "        continue\n",
    "    else:\n",
    "        right_advimg=right_advimg+1\n",
    "        print('adversarial class', np.argmax(fmodel.predictions(adversarial)))\n",
    "\n",
    "    \n",
    "    #===============abdm start (0.0)=========================================\n",
    "    im=adversarial\n",
    "    im = transform(im).numpy()\n",
    "    im = transform(im).numpy()\n",
    "    image_show=im\n",
    "    #im=im.resize(3,224,224)\n",
    "    print('ori image shape is :',im.shape)\n",
    "    print(\"===========================================================\")\n",
    "    im = im.reshape(1, 224, 224, 3)\n",
    "    im = im.astype('float32')\n",
    "    #print('img-over')\n",
    "    out_size = (224, 224)\n",
    "    batch = np.append(im, im, axis=0)\n",
    "    batch = np.append(batch, im, axis=0)\n",
    "    num_batch = 3 \n",
    "    x = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    x = tf.cast(batch, 'float32')\n",
    "    print('begin---')\n",
    "    with tf.variable_scope('abdm_0'):\n",
    "        n_fc = 6\n",
    "        w_fc1 = tf.Variable(tf.Variable(tf.zeros([224 * 224 * 3, n_fc]), name='W_fc1'))\n",
    "        initial = np.array([[0.5, 0, 0], [0, 0.5, 0]])\n",
    "        initial = initial.astype('float32')\n",
    "        initial = initial.flatten()  \n",
    "        b_fc1 = tf.Variable(initial_value=initial, name='b_fc1')   \n",
    "        h_fc1 = tf.matmul(tf.zeros([num_batch, 224 * 224 * 3]), w_fc1) + b_fc1\n",
    "        print(x, h_fc1, out_size)\n",
    "        h_trans = ABDM(x, h_fc1, out_size)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    y = sess.run(h_trans, feed_dict={x: batch})\n",
    "    stnimg_temp=transform(y[0]).numpy() \n",
    "    \n",
    "    adv_class=str(np.argmax(fmodel.predictions(stnimg_temp)))\n",
    "    orilabel=str(target)\n",
    "    print('adversarial class', np.argmax(fmodel.predictions(stnimg_temp)))\n",
    "    print('ori class', orilabel)\n",
    "    if(adv_class==orilabel):\n",
    "        # put images and labels into list\n",
    "        img_ori.append(image)\n",
    "        img_adv.append(adversarial)\n",
    "        img_label.append(target)\n",
    "        img_id.append(num)\n",
    "        stn_image.append(stnimg_temp)\n",
    "        print(len(img_id))\n",
    "        right_stnimg=right_stnimg+1\n",
    "    else:\n",
    "        print('can not use this img')\n",
    "        wrong_stnimg=wrong_stnimg+1\n",
    "        continue\n",
    "\n",
    "        \n",
    "ori_right=(100-wrong_oriimg)/100\n",
    "adv_right=(wrong_oriimg+wrong_advimg)/100\n",
    "stn_right=right_stnimg/100\n",
    "stn_right2=right_stnimg/(right_stnimg+wrong_stnimg)\n",
    "\n",
    "print('clean image Accuracy:  %.2f%%' % (ori_right * 100))\n",
    "print('adv image Accuracy:  %.2f%%' % (adv_right * 100))\n",
    "print('stn image Accuracy:  %.2f%%' % (stn_right * 100 ))      \n",
    "print('stn image Accuracy:  %.2f%%' % (stn_right2 * 100 ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ae",
   "language": "python",
   "name": "ae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
